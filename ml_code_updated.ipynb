{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "import models\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_truncate(dataset,batch_size,lookback_window, stride):\n",
    "    truncated_data = {}\n",
    "    # print(dataset)\n",
    "    for key,dataframe in dataset.items():\n",
    "        data_len = len(dataframe)\n",
    "        data_len_per_batch = ((batch_size)-1)*stride + lookback_window\n",
    "        num_batches = math.floor(data_len/data_len_per_batch)\n",
    "        if num_batches==0:\n",
    "            warnings.warn(\"Following dataset do not have enough samples: \"+key)\n",
    "            continue\n",
    "        ideal_data_len = num_batches*data_len_per_batch\n",
    "        test_case = data_len - ideal_data_len\n",
    "        if test_case!=0:\n",
    "            truncated_data[key] = dataframe.drop(dataframe.tail(test_case).index, axis = 0)\n",
    "            del test_case\n",
    "        else:\n",
    "            truncated_data[key] = dataframe\n",
    "            del test_case\n",
    "        del data_len\n",
    "    return truncated_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_loader(angles, imu, moment, len_cap, mode='FullKinematics', dynamics=None):\n",
    "    dataset = pd.DataFrame()\n",
    "    \n",
    "    if mode.lower()=='fullkinematics':\n",
    "        for col in angles.columns:\n",
    "            if 'knee' in col or 'hip_flexion' in col or 'ankle' in col:\n",
    "                dataset[col]=angles[col]\n",
    "        \n",
    "        for col in imu.columns:\n",
    "            if 'Shank' in col or 'Thigh' in col or 'Pelvis' in col:\n",
    "                dataset[col]=imu[col]\n",
    "    \n",
    "    else:\n",
    "        for col in angles.columns:\n",
    "            if 'ankle' in col:\n",
    "                dataset[col]=angles[col]\n",
    "        \n",
    "        for col in imu.columns:\n",
    "            if 'Shank' in col:\n",
    "                dataset[col]=imu[col]\n",
    "                \n",
    "    if dynamics is not None:\n",
    "        for col in dynamics.columns:\n",
    "            if col[0]=='L':\n",
    "                dataset[col]=dynamics[col]\n",
    "\n",
    "    dataset['ankle_moment'] = moment['ankle_angle_l_moment']\n",
    "    dataset.interpolate(method='spline',order=5,axis=0,limit=50,inplace=True)\n",
    "    dataset.dropna(axis=0, inplace=True)\n",
    "\n",
    "    if len(dataset)>len_cap:\n",
    "        n = len(dataset)-len_cap\n",
    "        dataset.drop(dataset.tail(n).index, inplace = True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_dict(subjects,tasks,batch_size=32,lookback_window=50, mode='FullKinematics', stride=1, get_dynamics=0, len_cap=4000, loading_mode='train'):\n",
    "    ##Generate filenames to look for based on subject and task list\n",
    "    file_list = {}\n",
    "    dataset = {}\n",
    "    address = 'Dataset/'\n",
    "    for sub in subjects:\n",
    "        for i in range(len(tasks)):\n",
    "            task=tasks[i]\n",
    "            temp = address + sub + '/' + task + '/' + sub + '_' + task + '_'\n",
    "            temp_a = temp + 'angle.csv'\n",
    "            temp_i = temp + 'imu_real.csv'\n",
    "            temp_m = temp + 'moment_filt.csv'\n",
    "            if get_dynamics==0:\n",
    "                file_list[sub + '_' + task] = [temp_a,temp_i,temp_m]\n",
    "                del temp, temp_a, temp_i, temp_m\n",
    "            else:\n",
    "                temp_d = temp + 'insole_sim.csv'\n",
    "                file_list[sub + '_' + task] = [temp_a,temp_i,temp_m,temp_d]\n",
    "                del temp, temp_a, temp_i, temp_m, temp_d\n",
    "        del task\n",
    "    file_names = list(file_list.keys())\n",
    "    \n",
    "    ##Check of the dataset for the given configuration of subject and task exist in the system\n",
    "    checker = '' \n",
    "    for item in file_names:\n",
    "        checker+=item\n",
    "    checker=checker+str(batch_size)+str(get_dynamics)+str(lookback_window)+str(stride)+mode \n",
    "    if loading_mode.lower()=='train':\n",
    "        with open('Data/train_info.txt','r+') as info_file:\n",
    "            line=''\n",
    "            for line in info_file:\n",
    "                pass\n",
    "            info = line.rstrip()\n",
    "            if info==checker:\n",
    "                return None\n",
    "            else:\n",
    "                info_file.write('\\n'+checker)\n",
    "    else:\n",
    "        with open('Data/test_info.txt','r+') as info_file:\n",
    "            line=''\n",
    "            for line in info_file:\n",
    "                pass\n",
    "            info = line.rstrip()\n",
    "            if info==checker:\n",
    "                return None\n",
    "            else:\n",
    "                info_file.write('\\n'+checker)\n",
    "\n",
    "    ##if the file do not exist then it will generate the dataset dictionary\n",
    "    for i in range(len(file_list)):\n",
    "        key = file_names[i]\n",
    "        angle = pd.read_csv(file_list[key][0])\n",
    "        imu = pd.read_csv(file_list[key][1])\n",
    "        moment = pd.read_csv(file_list[key][2])\n",
    "        \n",
    "        if get_dynamics==0:\n",
    "            dataset[key] = dataframe_loader(angle,imu,moment,len_cap,mode=mode)\n",
    "            del key, angle, imu, moment\n",
    "        else:\n",
    "            forces = pd.read_csv(file_list[key][3])\n",
    "            dataset[key] = dataframe_loader(angle,imu,moment,len_cap,mode=mode,dynamics=forces)\n",
    "            del key, angle, imu, moment, forces\n",
    "            \n",
    "    del file_list, file_names, checker\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stack_data(dataset, batch_size, lookback_window, stride, mode='train'):\n",
    "    truncated_dataset = dataset_truncate(dataset, batch_size, lookback_window, stride)\n",
    "    training_dataset = pd.DataFrame()\n",
    "    for key, dataframe in truncated_dataset.items():\n",
    "        training_dataset = pd.concat([training_dataset,dataframe])\n",
    "    del truncated_dataset\n",
    "    training_dataset=training_dataset.to_numpy()\n",
    "    if mode=='train':\n",
    "        scaler = MinMaxScaler()\n",
    "        training_dataset[:,:-1]=scaler.fit_transform(training_dataset[:,:-1])\n",
    "        # Save the parameters to a file\n",
    "        with open('Data/scaling_params.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        with open('Data/scaling_params.pkl', 'rb') as f:\n",
    "            scaler=pickle.load(f)\n",
    "            training_dataset[:,:-1]=scaler.transform(training_dataset[:,:-1])\n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "\n",
    "def load_data(subject,task,batch_size=32,lookback_window=50, stride=1,with_dynamics=0, data_len_cap=4000, mode=\"FullKinematics\", loading_mode='train'):\n",
    "\n",
    "    dataset_dict = generate_data_dict(subject, task, get_dynamics=with_dynamics, batch_size=batch_size, lookback_window=lookback_window, stride=stride,mode=mode, len_cap=data_len_cap, loading_mode=loading_mode)\n",
    "    if dataset_dict==None:\n",
    "        print(\"Loading local stored data\")\n",
    "        if loading_mode.lower()=='train':\n",
    "            dataset = np.load('Data/training_dataset.npy')\n",
    "        else:\n",
    "            dataset = np.load('Data/testing_dataset.npy')\n",
    "    else:\n",
    "        print('creating_data')\n",
    "        dataset=stack_data(dataset_dict, batch_size=batch_size,lookback_window=lookback_window,stride=stride,mode=loading_mode)\n",
    "        if loading_mode.lower()=='train':\n",
    "            np.save('Data/training_dataset.npy',dataset)\n",
    "        else:\n",
    "            np.save('Data/testing_dataset.npy',dataset)\n",
    "    feature_count = np.shape(dataset)[1]-1\n",
    "    print('Sample Points= ', np.shape(dataset)[0])\n",
    "    print('Feature_Count= ', feature_count)\n",
    "    del dataset_dict\n",
    "    return dataset,feature_count\n",
    "\n",
    "def reshape_IO(inputs,outputs, window_size=100, effective_window=1, skip_sz=5):\n",
    "    data_length = len(inputs)\n",
    "    output_length = window_size-effective_window+1\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(window_size,data_length,skip_sz):\n",
    "        temp = inputs[i-window_size:i,:]\n",
    "        temp2 = outputs[i-output_length:i]\n",
    "        X.append(temp)\n",
    "        Y.append(temp2)\n",
    "    X= np.array(X)\n",
    "    Y= np.array(Y)\n",
    "    del data_length\n",
    "    return X,Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(pred_output,test_output,name='LSTM',time_window=3,init=1, graph='on'):  #prediction plotter\n",
    "    # print(pred_output.shape,test_output.shape)\n",
    "    rmse = mean_squared_error(pred_output,test_output)**0.5\n",
    "    plot_add = 'Plots/'\n",
    "    if graph=='on':\n",
    "        unique_id = uuid.uuid1()\n",
    "        data_pts = int(200*time_window)\n",
    "        time = np.linspace(0, time_window, data_pts)\n",
    "        x=1\n",
    "        k=init\n",
    "        plt.plot(time,pred_output[k*x:k*x+data_pts], time,test_output[k*x:k*x+data_pts],'--')\n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.ylabel(\"Ankle Torque (N.m/Kg)\")\n",
    "        plt.title(\"%s | RMSE: %f\" %(name,rmse))\n",
    "        plt.legend(['Predicted','True value'])\n",
    "        plt.savefig(plot_add+f\"plot_{unique_id}.png\")\n",
    "        plt.show()\n",
    "    return rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz_train = 32\n",
    "lookback_train = 100\n",
    "stride_train = 3\n",
    "batch_sz_test = 32\n",
    "lookback_test = 100\n",
    "stride_test = 6\n",
    "num_epochs=25\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_importance(X_train, y_train):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_regression, k='all')\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    scores = []\n",
    "    for i in range(len(fs.scores_)):\n",
    "        scores.append(fs.scores_[i])\n",
    "    plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "    plt.show()\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Shortlisting - Feed Forward Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_task_generator(task_list,train_task_idx,preselected_tasks_idx=[0,1]):\n",
    "    train_task = []\n",
    "    holdout_task = task_list.copy()\n",
    "    if len(preselected_tasks_idx)!=0:\n",
    "        for item in preselected_tasks_idx:\n",
    "            train_task.append(task_list[item])\n",
    "            holdout_task.remove(task_list[item])\n",
    "\n",
    "    if train_task_idx==None:\n",
    "        return train_task, holdout_task  \n",
    "          \n",
    "    if train_task_idx not in preselected_tasks_idx:\n",
    "        train_task.append(task_list[train_task_idx])\n",
    "        holdout_task.remove(task_list[train_task_idx])\n",
    "        \n",
    "    return train_task, holdout_task\n",
    "\n",
    "def test_model_perf(train_subject, train_task, test_subject, holdout_task, with_dynamics=0, graph_switch='off'):\n",
    "    filepath=\"Model_weights/best_weights.h5\"\n",
    "    train_dataset,feature_count = load_data(train_subject,train_task,batch_size=batch_sz_train,lookback_window=lookback_train, stride=stride_train,with_dynamics=with_dynamics, data_len_cap=4000, loading_mode='train')\n",
    "    trainX,trainY=reshape_IO(train_dataset[:,:-1],train_dataset[:,-1],window_size=lookback_train,skip_sz=stride_train)\n",
    "    test_dataset,feature_count = load_data(test_subject,holdout_task,batch_size=batch_sz_test,lookback_window=lookback_test, stride=stride_test,with_dynamics=with_dynamics, data_len_cap=1000, loading_mode='test')\n",
    "    testX,testY = reshape_IO(test_dataset[:,:-1], test_dataset[:,-1], window_size=lookback_test,skip_sz=stride_test)\n",
    "    model=models.gen_lstm_model(feature_count)\n",
    "    history=model.fit(trainX, trainY, batch_size=batch_sz_train, validation_data=(testX,testY), epochs=num_epochs)\n",
    "    mse = min(history.history['val_mse'])\n",
    "    return mse\n",
    "\n",
    "\n",
    "## load the list of all the available tasks\n",
    "with open('Data/task_list.csv', 'r') as tasks:\n",
    "    task_list = tasks.read().splitlines()\n",
    "\n",
    "#Define the list of subjects for testing and training dataset\n",
    "train_subject =  ['AB01', 'AB02', 'AB03']\n",
    "test_subject =  ['AB05']\n",
    "\n",
    "finalized_tasks_idx = [2,6,7,13,15] ## define the indexes based on previous experiments if no previous experiment initialize empty array\n",
    "finalized_tasks = [] ## create a list with name of finalized tasks\n",
    "for item in finalized_tasks_idx:\n",
    "    finalized_tasks.append(task_list[item])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual task performance evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_num_train_task = 5\n",
    "num_req_iterations = req_num_train_task-len(finalized_tasks_idx)\n",
    "\n",
    "for i in range(num_req_iterations):\n",
    "    locally_best_task_idx=0\n",
    "    key=10\n",
    "    for j in range(len(task_list)):\n",
    "        print(i*len(task_list)+j+1, 'of', num_req_iterations*len(task_list))\n",
    "        if j not in finalized_tasks_idx:\n",
    "            train_task, holdout_task = training_task_generator(task_list,train_task_idx=j,preselected_tasks_idx=finalized_tasks_idx)\n",
    "            mse = test_model_perf(train_subject,train_task,test_subject,holdout_task,with_dynamics=1)\n",
    "            if mse<key:\n",
    "                key=mse\n",
    "                locally_best_task_idx = j\n",
    "            with open('feed_forward_task_selection.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([mse, task_list[j],finalized_tasks])\n",
    "    finalized_tasks_idx.append(locally_best_task_idx)\n",
    "    finalized_tasks.append(task_list[locally_best_task_idx])\n",
    "\n",
    "print(finalized_tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the list of all the available tasks\n",
    "with open('Data/task_list.csv', 'r') as tasks:\n",
    "    task_list = tasks.read().splitlines()\n",
    "\n",
    "#Define the list of subjects for testing and training dataset\n",
    "train_subject =  ['AB01', 'AB02', 'AB03']\n",
    "test_subject =  ['AB01', 'AB02', 'AB03']\n",
    "\n",
    "finalized_tasks_idx = [2,6,7,13,15,23] ## define the indexes based on previous experiments if no previous experiment initialize empty array\n",
    "\n",
    "train_task, holdout_task = training_task_generator(task_list,train_task_idx=None,preselected_tasks_idx=finalized_tasks_idx)\n",
    "## Load dataset\n",
    "train_dataset,feature_count = load_data(train_subject,train_task,batch_size=batch_sz_train,lookback_window=lookback_train, stride=stride_train,with_dynamics=1, data_len_cap=4000, mode=\"Exomode\", loading_mode='train')\n",
    "trainX,trainY=reshape_IO(train_dataset[:,:-1],train_dataset[:,-1],window_size=lookback_train,effective_window=lookback_train, skip_sz=stride_train)\n",
    "\n",
    "val_dataset,feature_count = load_data(test_subject,holdout_task,batch_size=batch_sz_test,lookback_window=lookback_test, stride=stride_test,with_dynamics=1, data_len_cap=4000, mode=\"Exomode\", loading_mode='test')\n",
    "valX,valY = reshape_IO(val_dataset[:,:-1], val_dataset[:,-1], window_size=lookback_test, effective_window=lookback_test, skip_sz=stride_test)\n",
    "\n",
    "# del train_dataset, test_dataset\n",
    "\n",
    "print(\"Training Data Shape\",trainX.shape, trainY.shape)\n",
    "print(\"Validation Data Shape\",valX.shape, valY.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_filepath = 'Model_weights/best_weights_lstm.h5'\n",
    "lstm_model = models.gen_lstm_model(features=feature_count, sequence_length=lookback_train)\n",
    "history_lstm = lstm_model.fit(trainX,trainY, validation_data=(valX,valY),epochs=num_epochs, batch_size=32, callbacks=[models.gen_checkpoint(checkpoint_filepath=lstm_filepath)])\n",
    "lstm_model.load_weights(lstm_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_filepath = 'Model_weights/best_weights_fcnn.h5'\n",
    "fcnn_model = models.gen_FCNN_model(feature_count)\n",
    "# history_fcnn=fcnn_model.fit(train_dataset[:,:-1],train_dataset[:,-1], validation_data=(val_dataset[:,:-1],val_dataset[:,-1]), epochs=num_epochs, callbacks=[models.gen_checkpoint(checkpoint_filepath=fcnn_filepath)], batch_size=32)\n",
    "fcnn_model.load_weights(fcnn_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(lstm_model)\n",
    "shap_values = explainer(train_dataset[:,:-1])\n",
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_filepath = 'Model_weights/best_weights_tcn.h5'\n",
    "tcn_model, effective_window = models.gen_tcn_model(features=feature_count, num_filters=64, kernel_size=3, num_conv_layers=5)\n",
    "trainX_tcn, trainY_tcn = reshape_IO(train_dataset[:,:-1],train_dataset[:,-1],window_size=500,skip_sz=300,effective_window=effective_window)\n",
    "valX_tcn, valY_tcn = reshape_IO(val_dataset[:,:-1],val_dataset[:,-1],window_size=500,skip_sz=300,effective_window=effective_window)\n",
    "print(trainX_tcn.shape,trainY_tcn.shape)\n",
    "history_tcn = tcn_model.fit(trainX_tcn,trainY_tcn, validation_data=(valX_tcn,valY_tcn),epochs=600, callbacks=[models.gen_checkpoint(checkpoint_filepath=tcn_filepath)])\n",
    "valY_tcn.reshape(-1)\n",
    "tcn_model.load_weights(tcn_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model on holdout_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results on holdout-tasks relevant to exoskeleton\n",
    "with open('Data/test_task_list.csv', 'r') as tasks:\n",
    "    test_task_list = tasks.read().splitlines()\n",
    "\n",
    "## Test results on all holdout-tasks\n",
    "# test_task_list = holdout_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_FCNN = []\n",
    "rmse_TCN = []\n",
    "rmse_LSTM = []\n",
    "holdout_tasks = []\n",
    "test_subject=['AB05']\n",
    "for item in test_task_list:\n",
    "    if item not in train_task:\n",
    "        print\n",
    "        test_task=[item]\n",
    "        print(item)\n",
    "        \n",
    "        test_dataset,feature_count = load_data(test_subject,test_task,batch_size=32,lookback_window=100, stride=1,with_dynamics=1, data_len_cap=4000, mode=\"Exomode\", loading_mode='test')\n",
    "        if test_dataset.shape[0]>=500:\n",
    "            testX,testY = reshape_IO(test_dataset[:,:-1], test_dataset[:,-1], effective_window=100, window_size=100, skip_sz=1)\n",
    "            testX_tcn,testY_tcn = reshape_IO(test_dataset[:,:-1], test_dataset[:,-1], effective_window=effective_window, window_size=500, skip_sz=500-effective_window)\n",
    "            predY_fcnn = fcnn_model.predict(test_dataset[:,:-1]).reshape(-1)\n",
    "            predY_LSTM = lstm_model.predict(testX).reshape(-1)\n",
    "            predY_tcn = tcn_model.predict(testX_tcn).reshape(-1)\n",
    "            testY = testY.reshape(-1)\n",
    "            testY_tcn = testY_tcn.reshape(-1)\n",
    "            temp1=predictor(predY_fcnn,test_dataset[:,-1],'fcnn_'+item,graph='on', time_window=2,init=50)\n",
    "            temp2= predictor(predY_LSTM,testY,'lstm_'+item,graph='on', time_window=2,init=50)\n",
    "            temp3= predictor(predY_tcn,testY_tcn,'tcn_'+item,graph='on', time_window=2,init=50)\n",
    "            holdout_tasks.append(item)\n",
    "            rmse_FCNN.append(temp1)\n",
    "            rmse_LSTM.append(temp2)\n",
    "            rmse_TCN.append(temp3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot evaluation results on Holdout Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_FCNN_rmse = np.average(rmse_FCNN)\n",
    "avg_LSTM_rmse = np.average(rmse_LSTM)\n",
    "avg_TCN_rmse = np.average(rmse_TCN)\n",
    "print('Average RMSE FCNN: ', avg_FCNN_rmse)\n",
    "print('Average RMSE LSTM: ', avg_LSTM_rmse)\n",
    "print('Average RMSE TCN: ', avg_TCN_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(holdout_tasks))\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the three data series as bar charts, with different colors\n",
    "ax.bar(x - 0.2, rmse_FCNN, color='red', width=0.2, align='center')\n",
    "ax.bar(x, rmse_LSTM, color='green', width=0.2, align='center')\n",
    "ax.bar(x + 0.2, rmse_TCN, color='blue', width=0.2, align='center')\n",
    "\n",
    "# Set the x-axis labels and title\n",
    "ax.set_xlabel('Ambulation modes')\n",
    "ax.set_ylabel('RMSE (N.m/Kg)')\n",
    "ax.set_title('RMSE of hold-out ambulation modes for test subject')\n",
    "\n",
    "# Set the x-axis ticks to show the sample points\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(holdout_tasks, rotation='vertical')\n",
    "\n",
    "# Set the legend for the three data series\n",
    "ax.legend(['FCNN', 'LSTM', 'TCN'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
